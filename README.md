# News-Artcles-ETL_Pipeline


###### Completed My Wikipedia Pageviews ETL + Analytics Project!

Over the past few days, I built an end-to-end ETL pipeline that automatically collects the top 1,000 most-viewed Wikipedia articles, cleans and transforms the data, and loads everything into a PostgreSQL database.

###### Tech Stack: Python, Requests, Pandas, PostgreSQL
 Visualization: Power BI

###### What I built:

* A modular ETL pipeline (extract - transform - load)

* Real-time data extraction using the Wikimedia REST API

* Data cleaning (removing system pages, URL generation, category extraction)

* A structured PostgreSQL database for long-term analytics

* Power BI dashboards to visualize trends, top pages, daily view counts, and categories

###### Key Highlights:

* Automated ingestion of fresh Wikipedia traffic data

* Clean, analysis-ready dataset

* Interactive insights in Power BI

* Modular pipeline that can be extended or scheduled anytime

This project really helped me sharpen my data engineering workflow and analytics visualization skills. Excited to keep improving and explore deeper insights from this dataset!
